{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb+srv://jobs:f4Uo1b3ziIAhpPMf@cluster0-79fkx.mongodb.net/jobs?retryWrites=true&w=majority')\n",
    "db=client.get_database(\"jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_data = list(db.datasciences_analysis.find({\"language\": \"en\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_misspelled_words(tokens, misspelled_words):\n",
    "    \n",
    "    tokens_l = tokens.split(\" \")\n",
    "    misspelled_words_l = misspelled_words.split(\" \")\n",
    "    cleaned_tokens = [token for token in tokens_l if token not in misspelled_words_l]\n",
    "    \n",
    "    return \" \".join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs = [remove_misspelled_words(job[\"tokens\"], job[\"misspelled_words\"]) for job in jobs_data]\n",
    "jobs = [job[\"tokens\"] for job in jobs_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three types of vectorizers, CountVectorizer and Hashing act as Bagg of Words (just counting) and Hashing with the trick to perform with memory, and as one hot encoder if it includes binnary =True. The third method is Tfid Vectorizer that takes into account all documents that promediates all the word not only in that document but in all documents. see book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(binary=True)\n",
    "# vectorizer = HashingVectorizer()\n",
    "vectorizer = HashingVectorizer(binary=True)\n",
    "# vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectorizer.fit_transform(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to high memory and that PCA cannot procede with dense matrix we use truncatedsvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_comp in [850]:\n",
    "    reductor = TruncatedSVD(n_components=n_comp, random_state=42)\n",
    "    reductor.fit(vectors)\n",
    "    print(n_comp, reductor.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_TSVD = reductor.transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_extra.cluster import KMedoids\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil = []\n",
    "range_list = [4,5,6,7,8,9,10,15,20,30,40,50,60,70,80,90]\n",
    "for n in range_list:\n",
    "    clustering = KMedoids(n_clusters=n, metric=\"euclidean\", random_state=0).fit(vectors_TSVD)\n",
    "    y_cocluster = clustering.labels_\n",
    "    sil_average = silhouette_score(vectors_TSVD, y_cocluster, metric='euclidean')\n",
    "    sil.append(sil_average)\n",
    "#     print(\"clusters: \", n, \"silhoutte: \", sil_average, Counter(y_cocluster))\n",
    "    print(\"clusters: \", n, \"silhoutte: \", sil_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [4,5,6,7,8,9,10,15,20,30,40,50,60,70,80,90]:\n",
    "    try:\n",
    "        visualizer = SilhouetteVisualizer(KMedoids(n_clusters=n, metric=\"cosine\", random_state=0))\n",
    "        visualizer.fit(vectors)\n",
    "        visualizer.poof()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range_list, sil)\n",
    "plt.grid()\n",
    "plt.xticks(range_list)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range_list, calis)\n",
    "plt.grid()\n",
    "plt.xticks(range_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmedoids = KMedoids(n_clusters=5, metric=\"cosine\", random_state=0).fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = list(zip(cleaned_jobs.keys(), kmedoids.labels_))\n",
    "clusters[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('.etl': venv)",
   "language": "python",
   "name": "python36864bitetlvenv92ebda5305554df0b42bc915c6bbe21f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
